{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tKRfy0T7-Yx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoFeatureExtractor, AutoModel\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === ViT feature extractor setup ===\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vit_model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n",
        "\n",
        "def extract_vit_features(image):\n",
        "    image = image.convert(\"RGB\")\n",
        "    inputs = extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        features = vit_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return features.cpu()\n",
        "\n",
        "# === Loop through dataset ===\n",
        "def load_dataset_and_features(dataset_path):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for label_str, label_val in {\"real\": 0, \"ai\": 1}.items():\n",
        "        folder = os.path.join(dataset_path, label_str)\n",
        "        for filename in tqdm(os.listdir(folder), desc=f\"Loading {label_str}\"):\n",
        "            try:\n",
        "                img_path = os.path.join(folder, filename)\n",
        "                image = Image.open(img_path).convert(\"RGB\")\n",
        "                feat = extract_vit_features(image)\n",
        "                features.append(feat)\n",
        "                labels.append(label_val)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error with {filename}: {e}\")\n",
        "    return torch.cat(features), torch.tensor(labels)\n",
        "\n",
        "X_train, y_train = load_dataset_and_features(\"dataset\")\n",
        "\n",
        "# === Define your model ===\n",
        "class FullModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(768, 384),\n",
        "            nn.BatchNorm1d(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(384, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# === Train ===\n",
        "model = FullModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for i in range(0, len(X_train), 16):\n",
        "        xb = X_train[i:i+16].to(device)\n",
        "        yb = y_train[i:i+16].float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb).squeeze()\n",
        "        loss = loss_fn(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"✅ Epoch {epoch+1} complete\")\n",
        "\n",
        "torch.save(model.state_dict(), \"full_model.pt\")\n",
        "print(\"✅ Trained model saved as full_model.pt\")\n"
      ]
    }
  ]
}
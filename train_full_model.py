# -*- coding: utf-8 -*-
"""train_full_model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFpNJBwxMyGRtO9XgseDGkzDXDI4d9SH
"""

import torch
import torch.nn as nn

class FullModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.projection = nn.Linear(768, 384)
        self.classifier = nn.Sequential(
            nn.Linear(384, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        x = self.projection(x)
        return self.classifier(x)

# Example training data placeholders:
X_train = torch.randn(100, 768)      # <- your ViT features go here
y_train = torch.randint(0, 2, (100,)) # <- your labels (0 or 1)

model = FullModel()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
loss_fn = nn.BCEWithLogitsLoss()

for epoch in range(5):  # short training for demo
    model.train()
    for i in range(0, len(X_train), 16):
        x_batch = X_train[i:i+16]
        y_batch = y_train[i:i+16].float()

        optimizer.zero_grad()
        outputs = model(x_batch).squeeze()
        loss = loss_fn(outputs, y_batch)
        loss.backward()
        optimizer.step()

torch.save(model.state_dict(), "full_model.pt")
print("âœ… full_model.pt saved!")

